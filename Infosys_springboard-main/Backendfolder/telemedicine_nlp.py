# -*- coding: utf-8 -*-
"""Telemedicine_NLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dSa3RVXR-Rc15WrCoTXSM87ISSRPqcFq

Preprocessing the data and saving into an csv file
"""

!pip install --upgrade --force-reinstall nltk

"""Preprocessing with using methods like removing stopword, Lemmatization, and tokanization"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import nltk

# Download required NLTK data
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

# Read the dataset
print("Reading dataset...")
df = pd.read_csv('/content/gpt-4.csv')

def simple_tokenize(text):
    """Simple tokenization by splitting on whitespace and punctuation"""
    return re.findall(r'\b\w+\b', text.lower())

def preprocess_text(text):
    """
    Preprocess text while preserving meaningful content
    """
    if not isinstance(text, str):
        return ''

    # Convert to lowercase
    text = text.lower()

    # Simple tokenization
    tokens = simple_tokenize(text)

    # Remove stopwords but keep important ones
    stop_words = set(stopwords.words('english'))
    important_words = {'no', 'not', 'very', 'can', 'cannot', 'medical', 'doctor', 'patient'}
    stop_words = stop_words - important_words
    tokens = [token for token in tokens if token not in stop_words]

    # Lemmatization
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(token) for token in tokens]

    # Join tokens back together
    processed_text = ' '.join(tokens)

    # Remove extra whitespace
    processed_text = ' '.join(processed_text.split())

    return processed_text

# Print example of original text
print("\nExample of original text:")
print(df['data'].iloc[0][:200], "...")

# Process the data
print("\nProcessing text data...")
df['processed_data'] = df['data'].apply(preprocess_text)
df['processed_conversation'] = df['conversation'].apply(preprocess_text)

# Print example of processed text
print("\nExample of processed text:")
print(df['processed_data'].iloc[0][:200], "...")

# Print statistics
print("\nText length statistics after preprocessing:")
print("\nData column:")
print(df['processed_data'].str.len().describe())
print("\nConversation column:")
print(df['processed_conversation'].str.len().describe())

# Check for empty strings
print("\nNumber of empty strings:")
print("Data column:", (df['processed_data'] == '').sum())
print("Conversation column:", (df['processed_conversation'] == '').sum())

# Save the preprocessed data
df.to_csv('preprocessed_gpt4.csv', index=False)
print("\nPreprocessed data saved to 'preprocessed_gpt4.csv'")

# Display sample rows to verify preprocessing
print("\nFirst few rows of processed data:")
print(df[['processed_data', 'processed_conversation']].head(2))

"""Loading the preprocessing csv file and applying the sentiment analysis"""

'''
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns
from textblob import TextBlob
# Read the preprocessed data
df = pd.read_csv('/content/preprocessed_gpt4.csv')
def clean_text_for_analysis(text):
    """Convert any non-string values to empty string"""
    if pd.isna(text) or not isinstance(text, str):
        return ''
    return text
def analyze_sentiment(text):
    """Analyze sentiment of text with proper error handling"""
    try:
        if pd.isna(text) or not isinstance(text, str):
            return 0
        return TextBlob(str(text)).sentiment.polarity
    except:
        return 0
def extract_features():
    print("Extracting features from text...")

    # Clean the text data
    df['processed_data'] = df['processed_data'].apply(clean_text_for_analysis)
    df['processed_conversation'] = df['processed_conversation'].apply(clean_text_for_analysis)

    # Remove any rows where processed text is empty
    print(f"Initial shape: {df.shape}")
    df_cleaned = df[df['processed_data'].str.len() > 0].copy()
    print(f"Shape after removing empty texts: {df_cleaned.shape}")

    # Add sentiment analysis
    print("Performing sentiment analysis...")
    df_cleaned['data_sentiment'] = df_cleaned['data'].apply(analyze_sentiment)
    df_cleaned['conversation_sentiment'] = df_cleaned['conversation'].apply(analyze_sentiment)

    # Create TF-IDF features
    print("Creating TF-IDF features...")
    tfidf = TfidfVectorizer(max_features=1000,
                           stop_words='english',
                           min_df=2,  # Minimum document frequency
                           max_df=0.95)  # Maximum document frequency

    data_tfidf = tfidf.fit_transform(df_cleaned['processed_data'])

    # Get top terms
    feature_names = tfidf.get_feature_names_out()
    top_terms = pd.DataFrame(data_tfidf.sum(axis=0).T,
                           index=feature_names,
                           columns=['weight']).sort_values('weight', ascending=False)

    return df_cleaned, data_tfidf, top_terms
def perform_topic_modeling(data_tfidf, n_topics=5):
    print(f"\nPerforming topic modeling with {n_topics} topics...")

    # LDA Topic Modeling
    lda = LatentDirichletAllocation(n_components=n_topics,
                                   random_state=42,
                                   max_iter=10,
                                   learning_method='online')
    lda_output = lda.fit_transform(data_tfidf)

    # Get dominant topics
    dominant_topics = np.argmax(lda_output, axis=1)

    return lda, lda_output, dominant_topics
def visualize_results(top_terms, df_cleaned):
    print("\nCreating visualizations...")
    # Plot top terms
    plt.figure(figsize=(12, 6))
    top_20_terms = top_terms.head(20)
    sns.barplot(x=top_20_terms['weight'], y=top_20_terms.index)
    plt.title('Top 20 Most Important Terms')
    plt.xlabel('TF-IDF Weight')
    plt.tight_layout()
    plt.show()

    # Plot sentiment distribution
    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    plt.hist(df_cleaned['data_sentiment'], bins=50, color='blue', alpha=0.7)
    plt.title('Sentiment Distribution - Data')
    plt.xlabel('Sentiment Score')
    plt.ylabel('Frequency')

    plt.subplot(1, 2, 2)
    plt.hist(df_cleaned['conversation_sentiment'], bins=50, color='green', alpha=0.7)
    plt.title('Sentiment Distribution - Conversation')
    plt.xlabel('Sentiment Score')
    plt.ylabel('Frequency')

    plt.tight_layout()
    plt.show()
def analyze_topics(lda, feature_names, n_top_words=10):
    print("\nTop words in each topic:")
    for topicidx, topic in enumerate(lda.components):
        top_words_idx = topic.argsort()[:-n_top_words-1:-1]
        top_words = [feature_names[i] for i in top_words_idx]
        print(f"\nTopic {topic_idx + 1}:")
        print(", ".join(top_words))
def main():
    try:
        # Extract features
        df_cleaned, data_tfidf, top_terms = extract_features()

        # Perform topic modeling
        lda, lda_output, dominant_topics = perform_topic_modeling(data_tfidf)

        # Visualize results
        visualize_results(top_terms, df_cleaned)

        # Analyze topics
        feature_names = TfidfVectorizer(max_features=1000,
                                      stop_words='english',
                                      min_df=2,
                                      max_df=0.95).fit(df_cleaned['processed_data']).get_feature_names_out()
        analyze_topics(lda, feature_names)

        # Add results to dataframe
        df_cleaned['dominant_topic'] = dominant_topics

        # Save enhanced dataset
        output_file = 'enhanced_gpt4.csv'
        df_cleaned.to_csv(output_file, index=False)
        print(f"\nEnhanced dataset saved to '{output_file}'")

        return df_cleaned, data_tfidf, lda_output

    except Exception as e:
        print(f"An error occurred in main processing: {e}")
        raise
if name == "main":
    # First install required packages if not already installed
    # !pip install textblob scikit-learn

    enhanced_df, tfidf_matrix, topic_distribution = main()
'''

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns
from textblob import TextBlob

# Read the preprocessed data
df = pd.read_csv('preprocessed_gpt4.csv')

def clean_text_for_analysis(text):
    """Convert any non-string values to empty string"""
    if pd.isna(text) or not isinstance(text, str):
        return ''
    return text

def analyze_sentiment(text):
    """Analyze sentiment of text with proper error handling"""
    try:
        if pd.isna(text) or not isinstance(text, str):
            return 0
        return TextBlob(str(text)).sentiment.polarity
    except:
        return 0

def extract_features():
    print("Extracting features from text...")

    # Clean the text data
    df['processed_data'] = df['processed_data'].apply(clean_text_for_analysis)
    df['processed_conversation'] = df['processed_conversation'].apply(clean_text_for_analysis)

    # Remove any rows where processed text is empty
    print(f"Initial shape: {df.shape}")
    df_cleaned = df[df['processed_data'].str.len() > 0].copy()
    print(f"Shape after removing empty texts: {df_cleaned.shape}")

    # Add sentiment analysis
    print("Performing sentiment analysis...")
    df_cleaned['data_sentiment'] = df_cleaned['data'].apply(analyze_sentiment)
    df_cleaned['conversation_sentiment'] = df_cleaned['conversation'].apply(analyze_sentiment)

    # Create TF-IDF features with adjusted parameters
    print("Creating TF-IDF features...")
    tfidf = TfidfVectorizer(max_features=1000,
                           stop_words=None,  # We've already removed stopwords
                           min_df=5,  # Increased minimum document frequency
                           max_df=0.9)  # Decreased maximum document frequency

    data_tfidf = tfidf.fit_transform(df_cleaned['processed_data'])

    # Get top terms
    feature_names = tfidf.get_feature_names_out()
    top_terms = pd.DataFrame(data_tfidf.sum(axis=0).T,
                           index=feature_names,
                           columns=['weight']).sort_values('weight', ascending=False)

    return df_cleaned, data_tfidf, top_terms

def perform_topic_modeling(data_tfidf, n_topics=5):
    print(f"\nPerforming topic modeling with {n_topics} topics...")

    # LDA Topic Modeling with adjusted parameters
    lda = LatentDirichletAllocation(n_components=n_topics,
                                   random_state=42,
                                   max_iter=20,  # Increased iterations
                                   learning_method='online',
                                   learning_offset=50.,
                                   batch_size=128)
    lda_output = lda.fit_transform(data_tfidf)

    # Get dominant topics
    dominant_topics = np.argmax(lda_output, axis=1)

    return lda, lda_output, dominant_topics

def visualize_results(top_terms, df_cleaned):
    print("\nCreating visualizations...")
    # Plot top terms
    plt.figure(figsize=(15, 8))
    top_20_terms = top_terms.head(20)
    sns.barplot(x=top_20_terms['weight'], y=top_20_terms.index)
    plt.title('Top 20 Most Important Terms')
    plt.xlabel('TF-IDF Weight')
    plt.tight_layout()
    plt.show()

    # Plot sentiment distribution
    plt.figure(figsize=(15, 6))

    plt.subplot(1, 2, 1)
    sns.histplot(data=df_cleaned['data_sentiment'], bins=50, color='blue', alpha=0.7)
    plt.title('Sentiment Distribution - Data')
    plt.xlabel('Sentiment Score')
    plt.ylabel('Frequency')

    plt.subplot(1, 2, 2)
    sns.histplot(data=df_cleaned['conversation_sentiment'], bins=50, color='green', alpha=0.7)
    plt.title('Sentiment Distribution - Conversation')
    plt.xlabel('Sentiment Score')
    plt.ylabel('Frequency')

    plt.tight_layout()
    plt.show()

def analyze_topics(lda, feature_names, n_top_words=10):
    print("\nTop words in each topic:")
    for topic_idx, topic in enumerate(lda.components_):
        top_words_idx = topic.argsort()[:-n_top_words-1:-1]
        top_words = [feature_names[i] for i in top_words_idx]
        print(f"\nTopic {topic_idx + 1}:")
        print(", ".join(top_words))

# Main execution
print("Starting analysis...")
enhanced_df, tfidf_matrix, topic_distribution = main()

"""Building the random forest model and saving it"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder

# Read the enhanced dataset
print("Loading enhanced dataset...")
df = pd.read_csv('enhanced_gpt4.csv')

def create_severity_labels(text):
    """Create severity labels based on text content"""
    text = text.lower()
    if any(word in text for word in ['severe', 'critical', 'emergency', 'urgent', 'icu']):
        return 'High'
    elif any(word in text for word in ['moderate', 'mild', 'stable']):
        return 'Moderate'
    else:
        return 'Low'

def prepare_data():
    print("Preparing data for classification...")

    # Create severity labels
    df['severity'] = df['processed_data'].apply(create_severity_labels)

    # Print class distribution
    print("\nClass distribution:")
    print(df['severity'].value_counts(normalize=True))

    # Split features and target
    X = df['processed_data']
    y = df['severity']

    # Encode labels
    le = LabelEncoder()
    y_encoded = le.fit_transform(y)

    # Split the data
    X_train, X_test, y_train, y_test = train_test_split(
        X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
    )

    # Create TF-IDF features
    tfidf = TfidfVectorizer(max_features=1000, min_df=5, max_df=0.9)
    X_train_tfidf = tfidf.fit_transform(X_train)
    X_test_tfidf = tfidf.transform(X_test)

    return X_train_tfidf, X_test_tfidf, y_train, y_test, le, tfidf

def train_model(X_train, y_train):
    print("\nTraining Random Forest model...")

    # Initialize and train the model
    rf = RandomForestClassifier(n_estimators=100,
                              max_depth=20,
                              min_samples_split=5,
                              random_state=42,
                              n_jobs=-1)
    rf.fit(X_train, y_train)

    return rf

def evaluate_model(model, X_test, y_test, label_encoder):
    print("\nEvaluating model performance...")

    # Make predictions
    y_pred = model.predict(X_test)

    # Print classification report
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred,
                              target_names=label_encoder.classes_))

    # Create confusion matrix
    cm = confusion_matrix(y_test, y_pred)

    # Plot confusion matrix
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=label_encoder.classes_,
                yticklabels=label_encoder.classes_)
    plt.title('Confusion Matrix')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.tight_layout()
    plt.show()

def analyze_feature_importance(model, tfidf):
    print("\nAnalyzing feature importance...")

    # Get feature importance
    feature_importance = pd.DataFrame({
        'feature': tfidf.get_feature_names_out(),
        'importance': model.feature_importances_
    })

    # Sort by importance
    feature_importance = feature_importance.sort_values('importance',
                                                      ascending=False)

    # Plot top 20 features
    plt.figure(figsize=(12, 6))
    sns.barplot(data=feature_importance.head(20),
                x='importance', y='feature')
    plt.title('Top 20 Most Important Features')
    plt.xlabel('Feature Importance')
    plt.tight_layout()
    plt.show()

    return feature_importance

def main():
    # Prepare data
    X_train, X_test, y_train, y_test, label_encoder, tfidf = prepare_data()

    # Train model
    model = train_model(X_train, y_train)

    # Evaluate model
    evaluate_model(model, X_test, y_test, label_encoder)

    # Analyze feature importance
    feature_importance = analyze_feature_importance(model, tfidf)

    # Save the model results
    results = {
        'model': model,
        'label_encoder': label_encoder,
        'tfidf': tfidf,
        'feature_importance': feature_importance
    }

    return results

# Run the analysis
results = main()

# Print available variables
import sys
variables = [var for var in dir() if not var.startswith('_')]
print("Available variables:", variables)

# Check what's in the results variable
print("Type of results:", type(results))
print("\nContents of results:")
for key in results.keys():
    print(f"- {key}")

# Check if we have the required components
if 'label_encoder' in locals():
    print("Label encoder is available")
if 'tfidf' in locals():
    print("TF-IDF vectorizer is available")

import joblib
import os
from datetime import datetime
import shutil

# Create directory for models if it doesn't exist
if not os.path.exists('models'):
    os.makedirs('models')

# Get timestamp for unique model name
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
model_dir = f'models/medical_classifier_{timestamp}'
os.makedirs(model_dir)

# Save all components from the results dictionary
model_path = os.path.join(model_dir, 'model.joblib')
tfidf_path = os.path.join(model_dir, 'tfidf_vectorizer.joblib')
encoder_path = os.path.join(model_dir, 'label_encoder.joblib')
importance_path = os.path.join(model_dir, 'feature_importance.joblib')

# Save the components
joblib.dump(results['model'], model_path)
joblib.dump(results['tfidf'], tfidf_path)
joblib.dump(results['label_encoder'], encoder_path)
joblib.dump(results['feature_importance'], importance_path)

# Save feature importance as CSV for easy viewing
results['feature_importance'].to_csv(os.path.join(model_dir, 'feature_importance.csv'))

print(f"Model and components saved in: {model_dir}")

# Create info file
info_path = os.path.join(model_dir, 'model_info.txt')
with open(info_path, 'w') as f:
    f.write(f"Model type: {type(results['model']).__name__}\n")
    f.write(f"Created at: {timestamp}\n")
    f.write(f"Number of features: {len(results['feature_importance'])}\n")
    f.write(f"Classes: {list(results['label_encoder'].classes_)}\n")

print("\nSaved files:")
print(f"- Model: {model_path}")
print(f"- TF-IDF vectorizer: {tfidf_path}")
print(f"- Label encoder: {encoder_path}")
print(f"- Feature importance: {importance_path}")
print(f"- Feature importance CSV: {os.path.join(model_dir, 'feature_importance.csv')}")
print(f"- Model info: {info_path}")

# Create a zip file of the model directory
zip_path = f'{model_dir}.zip'
shutil.make_archive(model_dir, 'zip', model_dir)
print(f"\nCreated zip archive: {zip_path}")

# Quick verification
print("\nVerifying saved files...")
try:
    # Try to load back the model to verify it was saved correctly
    loaded_model = joblib.load(model_path)
    loaded_tfidf = joblib.load(tfidf_path)
    loaded_encoder = joblib.load(encoder_path)
    print("‚úì All components loaded successfully!")
except Exception as e:
    print(f"Error during verification: {e}")

import joblib
import pandas as pd
import re

class MedicalTextClassifier:
    def __init__(self, model_dir):
        """Initialize the classifier with saved model components"""
        # Load the saved components
        self.model = joblib.load(f'{model_dir}/model.joblib')
        self.tfidf = joblib.load(f'{model_dir}/tfidf_vectorizer.joblib')
        self.label_encoder = joblib.load(f'{model_dir}/label_encoder.joblib')

        # Print loaded model info
        print("Model loaded successfully!")
        print(f"Available classes: {self.label_encoder.classes_}")

    def preprocess_text(self, text):
        """Preprocess the input text"""
        if not isinstance(text, str):
            return ''

        # Convert to lowercase
        text = text.lower()

        # Remove special characters but keep important punctuation
        text = re.sub(r'[^a-zA-Z\s.,!?]', ' ', text)

        # Remove extra whitespace
        text = ' '.join(text.split())

        return text

    def predict_single(self, text):
        """Make prediction for a single text input"""
        # Preprocess the text
        processed_text = self.preprocess_text(text)

        # Transform text using the saved TF-IDF vectorizer
        text_tfidf = self.tfidf.transform([processed_text])

        # Make prediction
        prediction_encoded = self.model.predict(text_tfidf)
        prediction_proba = self.model.predict_proba(text_tfidf)

        # Decode the prediction
        prediction = self.label_encoder.inverse_transform(prediction_encoded)

        # Get confidence scores
        class_probabilities = dict(zip(self.label_encoder.classes_,
                                     prediction_proba[0]))

        return {
            'prediction': prediction[0],
            'confidence_scores': class_probabilities,
            'processed_text': processed_text
        }

    def predict_batch(self, texts):
        """Make predictions for a batch of texts"""
        results = []
        for text in texts:
            results.append(self.predict_single(text))
        return results

# Example usage
def main():
    # Initialize the classifier with your saved model directory
    model_dir = '/content/models/medical_classifier_20250123_152858'  # Replace with your model's timestamp
    classifier = MedicalTextClassifier(model_dir)

    # Example single prediction
    sample_text = """
    Patient presented with severe shortness of breath, persistent cough,
    and fever of 39¬∞C. Oxygen saturation is at 92%.
    """

    print("\nMaking prediction for sample text:")
    result = classifier.predict_single(sample_text)
    print("\nPrediction:", result['prediction'])
    print("\nConfidence Scores:")
    for class_name, probability in result['confidence_scores'].items():
        print(f"{class_name}: {probability:.2%}")

    # Example batch prediction
    sample_texts = [
        "Patient shows mild symptoms with slight fever of 37.8¬∞C",
        "Emergency case: Patient with severe respiratory distress, requiring immediate ICU admission",
        "Follow-up visit shows stable vital signs, good recovery progress"
    ]

    print("\nMaking batch predictions:")
    results = classifier.predict_batch(sample_texts)

    for i, result in enumerate(results, 1):
        print(f"\nText {i}:")
        print(f"Prediction: {result['prediction']}")
        print("Confidence Scores:")
        for class_name, probability in result['confidence_scores'].items():
            print(f"{class_name}: {probability:.2%}")

if __name__ == "__main__":
    main()

"""Working app.py"""

!pip install pyngrok

!pip install streamlit

# Commented out IPython magic to ensure Python compatibility.
# %%writefile final.py
# import streamlit as st
# import joblib
# import pandas as pd
# import re
# import plotly.graph_objects as go
# from PIL import Image
# import io
# import base64
# 
# # Set page configuration
# st.set_page_config(
#     page_title="MediScan - Medical Text Analysis",
#     page_icon="üè•",
#     layout="wide",
#     initial_sidebar_state="expanded"
# )
# 
# # Custom CSS
# st.markdown("""
#     <style>
#         .main {
#             background-color: #f8f9fa;
#         }
#         .stButton>button {
#             background-color: #007bff;
#             color: white;
#             border-radius: 50px;
#             padding: 0.5rem 2rem;
#             border: none;
#         }
#         .stButton>button:hover {
#             background-color: #0056b3;
#         }
#         .severity-box {
#             padding: 20px;
#             border-radius: 15px;
#             margin: 10px 0;
#             box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
#         }
#         .high-severity {
#             background-color: black;
#             border-left: 5px solid #ff4444;
#         }
#         .moderate-severity {
#             background-color: black;
#             border-left: 5px solid #ffa500;
#         }
#         .low-severity {
#             background-color: black;
#             border-left: 5px solid #44ff44;
#         }
#         .stTextArea>div>div {
#             background-color: black;
#             border-radius: 10px;
#             border: 1px solid #e0e0e0;
#         }
#         .upload-box {
#             border: 2px dashed #007bff;
#             border-radius: 10px;
#             padding: 20px;
#             text-align: center;
#             margin: 20px 0;
#         }
#         .metric-card {
#             background-color: black;
#             padding: 15px;
#             border-radius: 10px;
#             box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
#         }
#     </style>
# """, unsafe_allow_html=True)
# 
# class MedicalTextClassifier:
#     def __init__(self, model_dir):
#         self.model = joblib.load(f'{model_dir}/model.joblib')
#         self.tfidf = joblib.load(f'{model_dir}/tfidf_vectorizer.joblib')
#         self.label_encoder = joblib.load(f'{model_dir}/label_encoder.joblib')
# 
#     def preprocess_text(self, text):
#         if not isinstance(text, str):
#             return ''
#         text = text.lower()
#         text = re.sub(r'[^a-zA-Z\s.,!?]', ' ', text)
#         return ' '.join(text.split())
# 
#     def predict_single(self, text):
#         processed_text = self.preprocess_text(text)
#         text_tfidf = self.tfidf.transform([processed_text])
#         prediction_encoded = self.model.predict(text_tfidf)
#         prediction_proba = self.model.predict_proba(text_tfidf)
#         prediction = self.label_encoder.inverse_transform(prediction_encoded)
#         class_probabilities = dict(zip(self.label_encoder.classes_, prediction_proba[0]))
#         return {
#             'prediction': prediction[0],
#             'confidence_scores': class_probabilities,
#             'processed_text': processed_text
#         }
# 
# def analyze_duration_context(text):
#     text_lower = text.lower()
#     duration_indicators = {
#         'days': r'(\d+)\s*days?',
#         'weeks': r'(\d+)\s*weeks?',
#         'months': r'(\d+)\s*months?'
#     }
# 
#     additional_context = []
# 
#     # Check for duration patterns
#     for period, pattern in duration_indicators.items():
#         matches = re.findall(pattern, text_lower)
#         if matches:
#             duration = int(matches[0])
#             if period == 'days' and duration > 7:
#                 additional_context.append(f"‚ö†Ô∏è Symptoms persisting for {duration} days may require medical evaluation.")
#             elif period == 'weeks' and duration > 1:
#                 additional_context.append(f"‚ö†Ô∏è Symptoms persisting for {duration} weeks require medical attention.")
#             elif period == 'months':
#                 additional_context.append(f"‚ö†Ô∏è Chronic condition lasting {duration} months - medical evaluation recommended.")
# 
#     # Check for specific symptoms that might need attention
#     symptoms_of_concern = {
#         'fever': "Monitor temperature and stay hydrated",
#         'chest pain': "Seek immediate medical attention for chest pain",
#         'difficulty breathing': "Monitor oxygen levels and breathing pattern",
#         'shortness of breath': "Monitor oxygen levels and breathing pattern",
#         'cold': "Monitor symptoms and seek medical attention if persisting beyond a week",
#         'cough': "Monitor cough progression and any changes in character",
#         'headache': "Monitor intensity and frequency of headaches",
#         'pain': "Track pain levels and any changes in intensity or location"
#     }
# 
#     for symptom, advice in symptoms_of_concern.items():
#         if symptom in text_lower:
#             additional_context.append(f"üîî {advice}")
# 
#     return additional_context
# 
# def create_gauge_chart(value, title, severity):
#     colors = {
#         'High': 'red',
#         'Moderate': 'orange',
#         'Low': 'green'
#     }
# 
#     fig = go.Figure(go.Indicator(
#         mode="gauge+number",
#         value=value * 100,
#         domain={'x': [0, 1], 'y': [0, 1]},
#         title={'text': title, 'font': {'size': 24}},
#         gauge={
#             'axis': {'range': [0, 100], 'tickwidth': 1},
#             'bar': {'color': colors.get(severity, 'blue')},
#             'bgcolor': "white",
#             'steps': [
#                 {'range': [0, 33], 'color': "rgba(255, 0, 0, 0.1)"},
#                 {'range': [33, 66], 'color': "rgba(255, 165, 0, 0.1)"},
#                 {'range': [66, 100], 'color': "rgba(0, 255, 0, 0.1)"}
#             ],
#             'threshold': {
#                 'line': {'color': colors.get(severity, 'blue'), 'width': 4},
#                 'thickness': 0.75,
#                 'value': value * 100
#             }
#         }
#     ))
# 
#     fig.update_layout(
#         height=250,
#         font={'size': 16},
#         margin=dict(l=20, r=20, t=40, b=20)
#     )
#     return fig
# 
# # Example cases
# example_cases = {
#     "Select an example case": "",
#     "Mild Cold Symptoms": """Patient presents with runny nose, mild cough, and sore throat for 2 days.
#     Temperature 37.2¬∞C. No difficulty breathing. Good appetite and energy levels.""",
#     "Persistent Cold (>1 week)": """Patient reports cold symptoms persisting for 10 days including nasal congestion,
#     intermittent cough, and fatigue. No fever. Some improvement noted but symptoms lingering. SpO2 98%, vital signs stable.""",
#     "Emergency Case": """Patient presented with severe chest pain, shortness of breath, and diaphoresis.
#     BP 180/100, HR 120, O2 sat 88%. ECG shows ST elevation. History of hypertension.""",
#     "Moderate COVID Symptoms": """Patient presents with fever (38.5¬∞C), persistent dry cough, fatigue, and mild
#     shortness of breath for 5 days. O2 sat 94%. Some difficulty with daily activities but alert and oriented.""",
# }
# def main():
#     # Sidebar
#     with st.sidebar:
#         st.image("https://img.icons8.com/color/96/000000/caduceus.png", width=100)
#         st.title("MediScan")
#         st.markdown("---")
#         st.markdown("""
#         ### About
#         MediScan uses advanced AI to analyze medical text and assess severity levels. Perfect for:
#         - Emergency triage
#         - Patient monitoring
#         - Clinical documentation
#         """)
#         st.markdown("---")
#         st.markdown("Made with ‚ù§Ô∏è by Group-1 Led by Saketh")
# 
#     # Initialize classifier
#     try:
#         classifier = MedicalTextClassifier('/content/models/medical_classifier_20250116_153450')
#         st.success("‚úÖ AI Model Ready", icon="‚ú®")
#     except Exception as e:
#         st.error(f"‚ö†Ô∏è System Error: {str(e)}")
#         return
# 
#     # Main content
#     tab1, tab2 = st.tabs(["üìù Single Case Analysis", "üìä Bulk Case Analysis"])
# 
#     # Single Case Analysis Tab
#     with tab1:
#         st.markdown("### üîç Analyze Individual Medical Case")
# 
#         if 'text_input' not in st.session_state:
#             st.session_state.text_input = ""
# 
#         # Example cases dropdown
#         selected_case = st.selectbox("üí° Quick Examples", options=list(example_cases.keys()))
# 
#         text_input = st.text_area(
#             "Enter Medical Notes:",
#             value=example_cases[selected_case] if selected_case != "Select an example case" else st.session_state.text_input,
#             height=150,
#             placeholder="Type or paste medical notes here..."
#         )
# 
#         col1, col2, col3 = st.columns([1, 2, 1])
#         with col2:
#             analyze_button = st.button("üîç Analyze Case", use_container_width=True)
# 
#         if analyze_button and text_input:
#             with st.spinner("üîÑ Analyzing medical case..."):
#                 result = classifier.predict_single(text_input)
# 
#             # Results section
#             st.markdown("### üìã Analysis Results")
# 
#             # Get severity and confidence
#             severity = result['prediction']
#             confidence = max(result['confidence_scores'].values())
# 
#             # Get additional context
#             context_notes = analyze_duration_context(text_input)
# 
#             # Display severity with context
#             severity_class = {
#                 'High': 'high-severity',
#                 'Moderate': 'moderate-severity',
#                 'Low': 'low-severity'
#             }.get(severity, '')
# 
#             st.markdown(f"""
#                 <div class='severity-box {severity_class}'>
#                     <h2>Case Severity: {severity}</h2>
#                     <p style='font-size: 1.2em;'>Confidence Level: {confidence:.1%}</p>
#                 </div>
#             """, unsafe_allow_html=True)
# 
#             # Display additional context if available
#             if context_notes:
#                 st.markdown("### üìå Additional Considerations")
#                 for note in context_notes:
#                     st.markdown(f"- {note}")
# 
#             if severity == "Low" and any("persisting" in note for note in context_notes):
#                 st.warning("While current symptoms suggest low severity, the duration of symptoms indicates that medical consultation may be advisable.")
# 
#             # Confidence scores visualization
#             st.markdown("### üìä Detailed Analysis")
#             for class_name, probability in result['confidence_scores'].items():
#                 fig = create_gauge_chart(probability, f"{class_name} Severity Level", class_name)
#                 st.plotly_chart(fig, use_container_width=True)
# 
#     # Bulk Case Analysis Tab
#     with tab2:
#         st.markdown("### üìÅ Bulk Case Analysis")
# 
#         # File uploader with custom styling
#         st.markdown("""
#             <div class='upload-box'>
#                 <h3>üì§ Upload Medical Cases</h3>
#                 <p>Drag and drop your CSV file here</p>
#             </div>
#         """, unsafe_allow_html=True)
# 
#         uploaded_file = st.file_uploader("", type=['csv'])
# 
#         if uploaded_file is not None:
#             try:
#                 df = pd.read_csv(uploaded_file)
#                 text_column = df.columns[0]  # Assume first column contains medical notes
# 
#                 st.markdown("### üìã Case Preview")
#                 st.dataframe(df.head(3), use_container_width=True)
# 
#                 col1, col2, col3 = st.columns([1, 2, 1])
#                 with col2:
#                     if st.button("üîç Analyze All Cases", use_container_width=True):
#                         progress_bar = st.progress(0)
#                         status_text = st.empty()
# 
#                         # Process each text and collect results
#                         results = []
#                         for i, text in enumerate(df[text_column]):
#                             result = classifier.predict_single(text)
#                             results.append(result)
#                             # Update progress
#                             progress = (i + 1) / len(df)
#                             progress_bar.progress(progress)
#                             status_text.text(f"Analyzing case {i+1} of {len(df)}")
# 
#                         # Clear progress indicators
#                         progress_bar.empty()
#                         status_text.empty()
# 
#                         # Add predictions to dataframe
#                         df['Severity_Level'] = [r['prediction'] for r in results]
#                         df['Confidence_Score'] = [max(r['confidence_scores'].values()) for r in results]
# 
#                         # Display summary metrics
#                         st.markdown("### üìä Analysis Summary")
#                         col1, col2, col3 = st.columns(3)
# 
#                         with col1:
#                             st.markdown("""
#                                 <div class='metric-card'>
#                                     <h3>Total Cases</h3>
#                                     <h2>{}</h2>
#                                 </div>
#                             """.format(len(df)), unsafe_allow_html=True)
# 
#                         with col2:
#                             high_severity = (df['Severity_Level'] == 'High').sum()
#                             st.markdown("""
#                                 <div class='metric-card'>
#                                     <h3>High Severity Cases</h3>
#                                     <h2 style='color: red;'>{}</h2>
#                                 </div>
#                             """.format(high_severity), unsafe_allow_html=True)
# 
#                         with col3:
#                             avg_confidence = df['Confidence_Score'].mean()
#                             st.markdown("""
#                                 <div class='metric-card'>
#                                     <h3>Avg. Confidence</h3>
#                                     <h2>{:.1%}</h2>
#                                 </div>
#                             """.format(avg_confidence), unsafe_allow_html=True)
# 
#                         # Display results
#                         st.markdown("### üìã Detailed Results")
#                         st.dataframe(df, use_container_width=True)
# 
#                         # Download results button
#                         csv = df.to_csv(index=False)
#                         st.download_button(
#                             label="üì• Download Complete Analysis",
#                             data=csv,
#                             file_name="medical_case_analysis.csv",
#                             mime="text/csv",
#                             use_container_width=True
#                         )
# 
#                         # Show distribution of predictions
#                         st.markdown("### üìä Severity Distribution")
#                         fig = go.Figure(data=[go.Pie(
#                             labels=df['Severity_Level'].value_counts().index,
#                             values=df['Severity_Level'].value_counts().values,
#                             hole=.3,
#                             marker_colors=['#ff4444', '#ffa500', '#44ff44']
#                         )])
#                         fig.update_layout(
#                             title="Distribution of Case Severities",
#                             height=400,
#                             showlegend=True
#                         )
#                         st.plotly_chart(fig, use_container_width=True)
# 
#             except Exception as e:
#                 st.error(f"‚ö†Ô∏è Error processing file: {str(e)}")
# 
# if __name__ == "__main__":
#     main()

!pip install pyngrok
!pip install streamlit

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import joblib
# import pandas as pd
# import re
# import plotly.graph_objects as go
# from PIL import Image
# import io
# import base64
# import requests
# import json
# import google.generativeai as genai
# 
# # Set page configuration
# st.set_page_config(
#     page_title="Medical Analysis Suite",
#     page_icon="üè•",
#     layout="wide",
#     initial_sidebar_state="expanded"
# )
# 
# # API Keys for Injury Analysis
# GEMINI_API_KEY = "AIzaSyBVL73UIy3dhEyP3OC4gCgMOQFgX2v6G8E"
# GOOGLE_SEARCH_API_KEY = "AIzaSyBkSmH7XzLcxi0HwGiYRoa6YW7poOfa5Fc"
# GOOGLE_CX = "95422de319fca48a0"
# 
# # Configure Gemini
# genai.configure(api_key=GEMINI_API_KEY)
# model = genai.GenerativeModel('gemini-1.5-flash')
# 
# # Custom CSS (from final.py)
# st.markdown("""
#     <style>
#         .main {
#             background-color: #f8f9fa;
#         }
#         .stButton>button {
#             background-color: #007bff;
#             color: white;
#             border-radius: 50px;
#             padding: 0.5rem 2rem;
#             border: none;
#         }
#         .stButton>button:hover {
#             background-color: #0056b3;
#         }
#         .severity-box {
#             padding: 20px;
#             border-radius: 15px;
#             margin: 10px 0;
#             box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
#         }
#         .high-severity {
#             background-color: black;
#             border-left: 5px solid #ff4444;
#         }
#         .moderate-severity {
#             background-color: black;
#             border-left: 5px solid #ffa500;
#         }
#         .low-severity {
#             background-color: black;
#             border-left: 5px solid #44ff44;
#         }
#         .stTextArea>div>div {
#             background-color: black;
#             border-radius: 10px;
#             border: 1px solid #e0e0e0;
#         }
#         .upload-box {
#             border: 2px dashed #007bff;
#             border-radius: 10px;
#             padding: 20px;
#             text-align: center;
#             margin: 20px 0;
#         }
#         .metric-card {
#             background-color: black;
#             padding: 15px;
#             border-radius: 10px;
#             box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
#         }
#     </style>
# """, unsafe_allow_html=True)
# 
# # Helper Functions
# def analyze_duration_context(text):
#     text_lower = text.lower()
#     duration_indicators = {
#         'days': r'(\d+)\s*days?',
#         'weeks': r'(\d+)\s*weeks?',
#         'months': r'(\d+)\s*months?'
#     }
# 
#     additional_context = []
# 
#     # Check for duration patterns
#     for period, pattern in duration_indicators.items():
#         matches = re.findall(pattern, text_lower)
#         if matches:
#             duration = int(matches[0])
#             if period == 'days' and duration > 7:
#                 additional_context.append(f"‚ö†Ô∏è Symptoms persisting for {duration} days may require medical evaluation.")
#             elif period == 'weeks' and duration > 1:
#                 additional_context.append(f"‚ö†Ô∏è Symptoms persisting for {duration} weeks require medical attention.")
#             elif period == 'months':
#                 additional_context.append(f"‚ö†Ô∏è Chronic condition lasting {duration} months - medical evaluation recommended.")
# 
#     # Check for specific symptoms that might need attention
#     symptoms_of_concern = {
#         'fever': "Monitor temperature and stay hydrated",
#         'chest pain': "Seek immediate medical attention for chest pain",
#         'difficulty breathing': "Monitor oxygen levels and breathing pattern",
#         'shortness of breath': "Monitor oxygen levels and breathing pattern",
#         'cold': "Monitor symptoms and seek medical attention if persisting beyond a week",
#         'cough': "Monitor cough progression and any changes in character",
#         'headache': "Monitor intensity and frequency of headaches",
#         'pain': "Track pain levels and any changes in intensity or location"
#     }
# 
#     for symptom, advice in symptoms_of_concern.items():
#         if symptom in text_lower:
#             additional_context.append(f"üîî {advice}")
# 
#     return additional_context
# 
# def create_gauge_chart(value, title, severity):
#     colors = {
#         'High': 'red',
#         'Moderate': 'orange',
#         'Low': 'green'
#     }
# 
#     fig = go.Figure(go.Indicator(
#         mode="gauge+number",
#         value=value * 100,
#         domain={'x': [0, 1], 'y': [0, 1]},
#         title={'text': title, 'font': {'size': 24}},
#         gauge={
#             'axis': {'range': [0, 100], 'tickwidth': 1},
#             'bar': {'color': colors.get(severity, 'blue')},
#             'bgcolor': "white",
#             'steps': [
#                 {'range': [0, 33], 'color': "rgba(255, 0, 0, 0.1)"},
#                 {'range': [33, 66], 'color': "rgba(255, 165, 0, 0.1)"},
#                 {'range': [66, 100], 'color': "rgba(0, 255, 0, 0.1)"}
#             ],
#             'threshold': {
#                 'line': {'color': colors.get(severity, 'blue'), 'width': 4},
#                 'thickness': 0.75,
#                 'value': value * 100
#             }
#         }
#     ))
# 
#     fig.update_layout(
#         height=250,
#         font={'size': 16},
#         margin=dict(l=20, r=20, t=40, b=20)
#     )
#     return fig
# 
# # Classes and Functions from final.py
# class MedicalTextClassifier:
#     def __init__(self, model_dir):
#         self.model = joblib.load(f'{model_dir}/model.joblib')
#         self.tfidf = joblib.load(f'{model_dir}/tfidf_vectorizer.joblib')
#         self.label_encoder = joblib.load(f'{model_dir}/label_encoder.joblib')
# 
#     def preprocess_text(self, text):
#         if not isinstance(text, str):
#             return ''
#         text = text.lower()
#         text = re.sub(r'[^a-zA-Z\s.,!?]', ' ', text)
#         return ' '.join(text.split())
# 
#     def predict_single(self, text):
#         processed_text = self.preprocess_text(text)
#         text_tfidf = self.tfidf.transform([processed_text])
#         prediction_encoded = self.model.predict(text_tfidf)
#         prediction_proba = self.model.predict_proba(text_tfidf)
#         prediction = self.label_encoder.inverse_transform(prediction_encoded)
#         class_probabilities = dict(zip(self.label_encoder.classes_, prediction_proba[0]))
#         return {
#             'prediction': prediction[0],
#             'confidence_scores': class_probabilities,
#             'processed_text': processed_text
#         }
# 
# # Functions from main.py
# def get_medicine_images(medicine_name):
#     """Fetch medicine image from Google Custom Search."""
#     try:
#         search_url = "https://www.googleapis.com/customsearch/v1"
#         params = {
#             'key': GOOGLE_SEARCH_API_KEY,
#             'cx': GOOGLE_CX,
#             'q': f"{medicine_name} medicine product",
#             'searchType': 'image',
#             'num': 1,
#             'imgSize': 'MEDIUM'
#         }
#         response = requests.get(search_url, params=params)
#         results = response.json()
# 
#         if 'items' in results:
#             return results['items'][0]['link']
#         return None
#     except Exception as e:
#         st.error(f"Error fetching image: {str(e)}")
#         return None
# 
# def analyze_injury(image_data):
#     """Analyze injury image using Gemini Vision API."""
#     prompt = """
#     Analyze this injury image and provide a CONCISE emergency response in the following format:
# 
#     INJURY TYPE:
#     [Brief one-line description of injury type]
# 
#     SEVERITY:
#     [Simple rating: Minor, Moderate, or Severe]
# 
#     IMMEDIATE ACTIONS:
#     1. [First step]
#     2. [Second step]
#     3. [Third step]
# 
#     REQUIRED ITEMS:
#     - [Specific medicine/item 1 with exact product name]
#     - [Specific medicine/item 2 with exact product name]
#     - [Specific medicine/item 3 with exact product name]
# 
#     SEEK MEDICAL HELP IF:
#     ‚Ä¢ [Critical condition 1]
#     ‚Ä¢ [Critical condition 2]
# 
#     Keep the response short and focused on immediate actions.
#     For medicines, provide specific product names that can be found in pharmacies.
#     """
#     try:
#         image_bytes = io.BytesIO()
#         image_data.save(image_bytes, format='PNG')
#         image_bytes = image_bytes.getvalue()
# 
#         vision_data = {'mime_type': 'image/png', 'data': image_bytes}
#         response = model.generate_content([prompt, vision_data])
#         analysis_text = response.text
# 
#         medicine_section = [section for section in analysis_text.split('\n\n') if 'REQUIRED ITEMS:' in section][0]
#         medicine_items = [item.replace('-', '').strip() for item in medicine_section.split('\n')[1:]]
# 
#         medicine_images = {}
#         for medicine in medicine_items:
#             image_url = get_medicine_images(medicine)
#             if image_url:
#                 medicine_images[medicine] = image_url
# 
#         return {
#             'analysis': analysis_text,
#             'medicine_images': medicine_images
#         }
#     except Exception as e:
#         return {'error': str(e)}
# 
# # Main Application
# def main():
#     # Sidebar
#     with st.sidebar:
#         st.image("https://img.icons8.com/color/96/000000/caduceus.png", width=100)
#         st.title("MediScan")
#         st.markdown("---")
# 
# 
#         # Feature Selection
#         selected_feature = st.radio(
#             "Choose Feature",
#             ["Medical Text Analysis", "Injury Analysis"]
#         )
# 
#         st.markdown("""
#         ### About
#         MediScan uses advanced AI to analyze medical text and assess severity levels. Perfect for:
#         - Emergency triage
#         - Patient monitoring
#         - Clinical documentation
#         """)
#         st.markdown("---")
#         st.markdown("Made with ‚ù§Ô∏è by Group-1 Led by Saketh")
# 
#     # Main content based on selected feature
#     if selected_feature == "Medical Text Analysis":
#         st.title("MediScan - Medical Text Analysis")
# 
#         try:
#             classifier = MedicalTextClassifier('/content/models/medical_classifier_20250116_153450')
#             st.success("‚úÖ AI Model Ready", icon="‚ú®")
# 
#             tab1, tab2 = st.tabs(["üìù Single Case Analysis", "üìä Bulk Case Analysis"])
# 
#             # Example cases from final.py
#             example_cases = {
#                 "Select an example case": "",
#                 "Mild Cold Symptoms": """Patient presents with runny nose, mild cough, and sore throat for 2 days.
#                 Temperature 37.2¬∞C. No difficulty breathing. Good appetite and energy levels.""",
#                 "Persistent Cold (>1 week)": """Patient reports cold symptoms persisting for 10 days including nasal congestion,
#                 intermittent cough, and fatigue. No fever. Some improvement noted but symptoms lingering. SpO2 98%, vital signs stable.""",
#                 "Emergency Case": """Patient presented with severe chest pain, shortness of breath, and diaphoresis.
#                 BP 180/100, HR 120, O2 sat 88%. ECG shows ST elevation. History of hypertension.""",
#                 "Moderate COVID Symptoms": """Patient presents with fever (38.5¬∞C), persistent dry cough, fatigue, and mild
#                 shortness of breath for 5 days. O2 sat 94%. Some difficulty with daily activities but alert and oriented.""",
#             }
# 
#             # Single Case Analysis Tab
#             with tab1:
#                 st.markdown("### üîç Analyze Individual Medical Case")
# 
#                 if 'text_input' not in st.session_state:
#                     st.session_state.text_input = ""
# 
#                 # Example cases dropdown
#                 selected_case = st.selectbox("üí° Quick Examples", options=list(example_cases.keys()))
# 
#                 text_input = st.text_area(
#                     "Enter Medical Notes:",
#                     value=example_cases[selected_case] if selected_case != "Select an example case" else st.session_state.text_input,
#                     height=150,
#                     placeholder="Type or paste medical notes here..."
#                 )
# 
#                 col1, col2, col3 = st.columns([1, 2, 1])
#                 with col2:
#                     analyze_button = st.button("üîç Analyze Case", use_container_width=True)
# 
#                 if analyze_button and text_input:
#                     with st.spinner("üîÑ Analyzing medical case..."):
#                         result = classifier.predict_single(text_input)
# 
#                     # Results section
#                     st.markdown("### üìã Analysis Results")
# 
#                     # Get severity and confidence
#                     severity = result['prediction']
#                     confidence = max(result['confidence_scores'].values())
# 
#                     # Get additional context
#                     context_notes = analyze_duration_context(text_input)
# 
#                     # Display severity with context
#                     severity_class = {
#                         'High': 'high-severity',
#                         'Moderate': 'moderate-severity',
#                         'Low': 'low-severity'
#                     }.get(severity, '')
# 
#                     st.markdown(f"""
#                         <div class='severity-box {severity_class}'>
#                             <h2>Case Severity: {severity}</h2>
#                             <p style='font-size: 1.2em;'>Confidence Level: {confidence:.1%}</p>
#                         </div>
#                     """, unsafe_allow_html=True)
# 
#                     # Display additional context if available
#                     if context_notes:
#                         st.markdown("### üìå Additional Considerations")
#                         for note in context_notes:
#                             st.markdown(f"- {note}")
# 
#                     if severity == "Low" and any("persisting" in note for note in context_notes):
#                         st.warning("While current symptoms suggest low severity, the duration of symptoms indicates that medical consultation may be advisable.")
# 
#                     # Confidence scores visualization
#                     st.markdown("### üìä Detailed Analysis")
#                     for class_name, probability in result['confidence_scores'].items():
#                         fig = create_gauge_chart(probability, f"{class_name} Severity Level", class_name)
#                         st.plotly_chart(fig, use_container_width=True)
# 
#             # Bulk Case Analysis Tab
#             with tab2:
#                 st.markdown("### üìÅ Bulk Case Analysis")
# 
#                 # File uploader with custom styling
#                 st.markdown("""
#                     <div class='upload-box'>
#                         <h3>üì§ Upload Medical Cases</h3>
#                         <p>Drag and drop your CSV file here</p>
#                     </div>
#                 """, unsafe_allow_html=True)
# 
#                 uploaded_file = st.file_uploader("", type=['csv'])
# 
#                 if uploaded_file is not None:
#                     try:
#                         df = pd.read_csv(uploaded_file)
#                         text_column = df.columns[0]  # Assume first column contains medical notes
# 
#                         st.markdown("### üìã Case Preview")
#                         st.dataframe(df.head(3), use_container_width=True)
# 
#                         col1, col2, col3 = st.columns([1, 2, 1])
#                         with col2:
#                             if st.button("üîç Analyze All Cases", use_container_width=True):
#                                 progress_bar = st.progress(0)
#                                 status_text = st.empty()
# 
#                                 # Process each text and collect results
#                                 results = []
#                                 for i, text in enumerate(df[text_column]):
#                                     result = classifier.predict_single(text)
#                                     results.append(result)
#                                     # Update progress
#                                     progress = (i + 1) / len(df)
#                                     progress_bar.progress(progress)
#                                     status_text.text(f"Analyzing case {i+1} of {len(df)}")
# 
#                                 # Clear progress indicators
#                                 progress_bar.empty()
#                                 status_text.empty()
# 
#                                 # Add predictions to dataframe
#                                 df['Severity_Level'] = [r['prediction'] for r in results]
#                                 df['Confidence_Score'] = [max(r['confidence_scores'].values()) for r in results]
# 
#                                 # Display summary metrics
#                                 st.markdown("### üìä Analysis Summary")
#                                 col1, col2, col3 = st.columns(3)
# 
#                                 with col1:
#                                     st.markdown("""
#                                         <div class='metric-card'>
#                                             <h3>Total Cases</h3>
#                                             <h2>{}</h2>
#                                         </div>
#                                     """.format(len(df)), unsafe_allow_html=True)
# 
#                                 with col2:
#                                     high_severity = (df['Severity_Level'] == 'High').sum()
#                                     st.markdown("""
#                                         <div class='metric-card'>
#                                             <h3>High Severity Cases</h3>
#                                             <h2 style='color: red;'>{}</h2>
#                                         </div>
#                                     """.format(high_severity), unsafe_allow_html=True)
# 
#                                 with col3:
#                                     avg_confidence = df['Confidence_Score'].mean()
#                                     st.markdown("""
#                                         <div class='metric-card'>
#                                             <h3>Avg. Confidence</h3>
#                                             <h2>{:.1%}</h2>
#                                         </div>
#                                     """.format(avg_confidence), unsafe_allow_html=True)
# 
#                                 # Display results
#                                 st.markdown("### üìã Detailed Results")
#                                 st.dataframe(df, use_container_width=True)
# 
#                                 # Download results button
#                                 csv = df.to_csv(index=False)
#                                 st.download_button(
#                                     label="üì• Download Complete Analysis",
#                                     data=csv,
#                                     file_name="medical_case_analysis.csv",
#                                     mime="text/csv",
#                                     use_container_width=True
#                                 )
# 
#                                 # Show distribution of predictions
#                                 st.markdown("### üìä Severity Distribution")
#                                 fig = go.Figure(data=[go.Pie(
#                                     labels=df['Severity_Level'].value_counts().index,
#                                     values=df['Severity_Level'].value_counts().values,
#                                     hole=.3,
#                                     marker_colors=['#ff4444', '#ffa500', '#44ff44']
#                                 )])
#                                 fig.update_layout(
#                                     title="Distribution of Case Severities",
#                                     height=400,
#                                     showlegend=True
#                                 )
#                                 st.plotly_chart(fig, use_container_width=True)
# 
#                     except Exception as e:
#                         st.error(f"‚ö†Ô∏è Error processing file: {str(e)}")
# 
#         except Exception as e:
#             st.error(f"‚ö†Ô∏è System Error: {str(e)}")
# 
#     else:  # Injury Analysis
#         st.title("Emergency Injury Analyzer")
#         st.write("Upload an injury photo to get an emergency response, including suggested actions and required items.")
# 
#         uploaded_image = st.file_uploader("Upload an image", type=["jpg", "jpeg", "png"])
# 
#         if uploaded_image:
#             img = Image.open(uploaded_image)
#             if img.mode == 'RGBA':
#                 img = img.convert('RGB')
# 
#             st.image(img, caption="Uploaded Image", use_column_width=True)
# 
#             if st.button("Analyze Injury"):
#                 st.write("Analyzing the image. Please wait...")
#                 result = analyze_injury(img)
# 
#                 if 'error' in result:
#                     st.error(f"Error: {result['error']}")
#                 else:
#                     analysis = result['analysis']
#                     medicine_images = result['medicine_images']
# 
#                     # Display analysis results
#                     sections = analysis.split('\n\n')
#                     for section in sections:
#                         if "INJURY TYPE:" in section:
#                             st.subheader("Injury Type")
#                             st.write(section.replace("INJURY TYPE:", "").strip())
#                         elif "IMMEDIATE ACTIONS:" in section:
#                             st.subheader("Immediate Actions")
#                             actions = section.split('\n')[1:]
#                             st.write("\n".join(f"- {action.strip()}" for action in actions))
#                         elif "REQUIRED ITEMS:" in section:
#                             st.subheader("Required Items")
#                             items = section.split('\n')[1:]
#                             for item in items:
#                                 item_name = item.replace('-', '').strip()
#                                 if item_name in medicine_images:
#                                     st.image(medicine_images[item_name], caption=item_name, width=150)
#                                 else:
#                                     st.write(f"- {item_name}")
#                         elif "SEEK MEDICAL HELP IF:" in section:
#                             st.subheader("Seek Medical Help If")
#                             warnings = section.split('\n')[1:]
#                             st.write("\n".join(f"- {warning.strip()}" for warning in warnings))
# 
# if __name__ == "__main__":
#     main()

from pyngrok import ngrok

# Set ngrok authentication token (replace with your actual token)
ngrok.set_auth_token("2rcDbR17wD7lsB54rfZM1YwoZYG_42SX36S32fzbPwDZnLAAp")

# Start Streamlit on port 8000
!nohup streamlit run app.py --server.port 8000 &

# Create a tunnel to port 8000
url = ngrok.connect(8000)  # Use an integer instead of a string
print(url)

import pandas as pd

# Create sample medical cases with more explicit medical terminology
medical_cases = [
    {
        "medical_notes": """EMERGENCY: Patient presents with acute myocardial infarction. Severe chest pain (10/10) with radiation to left arm.
        Critical vital signs: BP 180/110, Heart rate 135, O2 saturation 85%. ECG shows ST elevation in V1-V4.
        Patient requiring immediate cardiac intervention."""
    },
    {
        "medical_notes": """Routine follow-up visit for mild hypertension. Patient reports feeling well.
        Vital signs stable: BP 128/82, Heart rate 72, Temperature 36.8¬∞C. No complaints.
        Medication compliance good."""
    },
    {
        "medical_notes": """CRITICAL: Patient presented unconscious, Glasgow Coma Scale 6/15. Multiple trauma from MVA.
        Severe head injury with unequal pupils. BP 210/120, irregular heart rate 150.
        Immediate intubation required. Multiple fractures suspected."""
    },
    {
        "medical_notes": """Patient presents with mild upper respiratory symptoms.
        Reports occasional cough and nasal congestion for 2 days. Temperature 37.2¬∞C.
        O2 saturation 98%. No respiratory distress."""
    },
    {
        "medical_notes": """URGENT: Severe diabetic ketoacidosis. Blood glucose 678 mg/dL.
        Patient confused and severely dehydrated. Ketones strongly positive.
        pH 7.1. Requires immediate insulin therapy and ICU admission."""
    },
    {
        "medical_notes": """Moderate case: Patient 5 days post-appendectomy with surgical site infection.
        Temperature 38.5¬∞C, elevated WBC count. Wound showing purulent drainage.
        Requiring IV antibiotics but hemodynamically stable."""
    },
    {
        "medical_notes": """EMERGENCY: Acute respiratory failure. O2 saturation 82% on room air.
        Severe respiratory distress with accessory muscle use. Patient cyanotic.
        Requiring immediate ventilatory support. CODE BLUE initiated."""
    },
    {
        "medical_notes": """Minor trauma: Small laceration on finger from paper cut.
        Minimal bleeding, wound clean. No tendon involvement.
        Tetanus up to date. Simple dressing applied."""
    },
    {
        "medical_notes": """CRITICAL: Severe septic shock. BP 75/40 despite fluid resuscitation.
        Patient requiring multiple vasopressors. Lactate 8.5.
        Source suspected intra-abdominal. Immediate ICU care needed."""
    },
    {
        "medical_notes": """Follow-up visit for stable angina. Patient reports good response to medication.
        No recent chest pain episodes. Exercise tolerance improving.
        Vital signs within normal limits."""
    }
]

# Create DataFrame
df = pd.DataFrame(medical_cases)

# Save to CSV
df.to_csv('medical_cases.csv', index=False)

print("Expected severity distribution:")
print("\nHigh Severity Cases (4):")
print("1. Acute myocardial infarction with critical vitals")
print("2. Multiple trauma with GCS 6")
print("3. Severe DKA requiring ICU")
print("4. Septic shock with hypotension")

print("\nModerate Severity Cases (2):")
print("1. Post-operative infection")
print("2. Respiratory distress requiring intervention")

print("\nLow Severity Cases (4):")
print("1. Routine hypertension follow-up")
print("2. Mild upper respiratory infection")
print("3. Minor laceration")
print("4. Stable angina follow-up")

